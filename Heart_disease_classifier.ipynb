{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Heart_disease_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOECh6HpGo0h5VVXRr0M2eO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SrijanxxxSharma/Heart_disease_classification/blob/main/Heart_disease_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzqDN1kknUBy"
      },
      "source": [
        "# **Heart disease dataset**\n",
        "\n",
        "### This data set can be found in my Github repository as https://github.com/SrijanxxxSharma/Heart_disease_classification.\n",
        "\n",
        "Looking at the data it is quite clear that this is a classification problem(target column is a categorical column). 8 of its columns are categorical(0,1,2,3...) while remaining 5 are continuous numeric column.\n",
        "\n",
        "  Since we don't have a lot of features we won't use any DR(dimension reduction). We'll just scale 5 of our features which are continuous(standardizing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5xvQysxqSE9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "url = \"https://github.com/SrijanxxxSharma/Heart_disease_classification/raw/main/heart-disease-dataset.csv\"\n",
        "raw_data=pd.read_csv(url)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo0rUlh6UI0s"
      },
      "source": [
        "#Now we will use train test split for creating 3 sets\n",
        "1.Training set.(70% of data)\n",
        "\n",
        "2.Test set.(20% of data)\n",
        "\n",
        "3.Validation set.(10% of data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR3GSq6ZUeit"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target=raw_data.target\n",
        "features=raw_data.drop(\"target\",axis=1)\n",
        "\n",
        "# Spliting the data into train and remain sets\n",
        "X_train, X_remain, y_train, y_remain = train_test_split(features, target, test_size=0.30, random_state=42)\n",
        "\n",
        "#spliting test set into test and validation test\n",
        "\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_remain, y_remain, test_size=0.34, random_state=42)\n",
        "X_train.reset_index(inplace=True,drop=True)\n",
        "X_test.reset_index(inplace=True,drop=True)\n",
        "X_val.reset_index(inplace=True,drop=True)\n",
        "y_train=y_train.reset_index(drop=True)\n",
        "y_test=y_test.reset_index(drop=True)\n",
        "y_val=y_val.reset_index(drop=True)\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJkR5NwD4iiI"
      },
      "source": [
        "#preprocessing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Now we extract continuous variables and convert them to numpy array to scale them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6E3f-rKFb3o"
      },
      "source": [
        "def preprocess(raw_data):\n",
        "  #extracting continuous variables as list of numpy arrays  \n",
        "  continuous_data=[raw_data[members].to_numpy() for members in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']]\n",
        "  \n",
        "  #scaling\n",
        "  name=['Scaled_age', 'Scaled_trestbps', 'Scaled_chol', 'Scaled_thalach', 'Scaled_oldpeak']\n",
        "  i=0\n",
        "  for feature in continuous_data:\n",
        "    new=3*(feature-np.min(feature))/(np.max(feature)-np.min(feature))\n",
        "    #new=(4*feature-feature.mean())/feature.std()\n",
        "    raw_data=pd.concat([raw_data,pd.DataFrame(new, columns = [name[i]])], axis=1)\n",
        "    i+=1\n",
        "  columns=['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
        "  raw_data.drop(columns,inplace=True,axis=1)\n",
        "  return raw_data\n",
        "\n",
        "scaled_data = preprocess(X_train) "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBaOw2pjsVEl"
      },
      "source": [
        "## Error analysis\n",
        "We use mean squared error to analyse our model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I618hg7LslWf"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.metrics import mean_absolute_error as err\n",
        "def sq_err_per(original,predictions):\n",
        "  return 100*mse(original,predictions)\n",
        "\n",
        "def err_per(original,predictions):\n",
        "  return 100*err(original,predictions)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIkiv4RCcD1W"
      },
      "source": [
        "# Now Model\n",
        "Now we have nicely scaled data as scaled_data. We will use classifiers to train our Model.\n",
        "\n",
        "1.K Nearest Neighbour\n",
        "\n",
        "2.Logistic Regression\n",
        "\n",
        "3.Naïve Bayes\n",
        "\n",
        "4.Support Vector Machine\n",
        "\n",
        "5.Decision Tree\n",
        "\n",
        "6.Random Forest\n",
        "\n",
        "7.XGboost "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "symiA0ZAeAYX"
      },
      "source": [
        "## K NearestNeighbours(86.69% accuracy)\n",
        "\n",
        "This classifier has simple concept. It checks the neighbouring point's label using Eucledean distance. Here K is number if neighbours one wants to consider.\n",
        "\n",
        "#Curse of Dimensionality\n",
        "\n",
        "KNN performs better with a lower number of features than a large number of features. You can say that when the number of features increases than it requires more data. Increase in dimension also leads to the problem of overfitting. To avoid overfitting, the needed data will need to grow exponentially as you increase the number of dimensions. This problem of higher dimension is known as the Curse of Dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztZP9wDlfa-X",
        "outputId": "54e7b3f1-eeb2-4301-c429-e8d2a4adb2e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "# Import relavent model from sklearn\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "model = KNeighborsClassifier(n_neighbors=10)\n",
        "\n",
        "# Now fit in each point\n",
        "model.fit(scaled_data,y_train)\n",
        "# Now predict for test set but remember to preprocess it else it will be a disaster\n",
        "predictions = model.predict(preprocess(X_test))\n",
        "cm = metrics.confusion_matrix(y_test, predictions)#Confusion metrics\n",
        "\n",
        "#printing errors\n",
        "print((100-sq_err_per(y_test, predictions)))\n",
        "print((100-err_per(y_test, predictions)))\n",
        "print(cm)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "85.71428571428572\n",
            "85.71428571428572\n",
            "[[96 16]\n",
            " [13 78]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0qZsurXwJJ1"
      },
      "source": [
        "# Logistic regressor(79.8% accuracy)\n",
        "\n",
        "Logistic Regression is used when the dependent variable(target) is categorical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN3oeDL9B7kb",
        "outputId": "c6a5eda3-99c5-40b8-d4f6-da8a8dd8b983",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "model2=LogisticRegression()\n",
        "model2.fit(scaled_data,y_train)\n",
        "\n",
        "# Now predict for test set but remember to preprocess it else it will be a disaster\n",
        "predictions = model2.predict(preprocess(X_test))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, predictions)#Confusion metrics\n",
        "\n",
        "#printing errors\n",
        "print((100-sq_err_per(y_test, predictions)))\n",
        "print((100-err_per(y_test, predictions)))\n",
        "print(cm)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79.80295566502463\n",
            "79.80295566502463\n",
            "[[79 33]\n",
            " [ 8 83]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN7i4IJOFxyC"
      },
      "source": [
        "# Naïve Bayes(78.3% accuracy)\n",
        "A Naive Bayes classifier is a probabilistic machine learning model that’s used for classification task. The crux of the classifier is based on the Bayes theorem.\n",
        "\n",
        "We make two assumptions here, one as stated above we consider that these predictors are independent. That is, if the temperature is hot, it does not necessarily mean that the humidity is high. Another assumption made here is that all the predictors have an equal effect on the outcome. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co7rTEQTGKPi",
        "outputId": "af3813ff-4629-4942-860a-fc73de5d4992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "model3 = GaussianNB()\n",
        "model3.fit(scaled_data,y_train)\n",
        "\n",
        "\n",
        "\n",
        "# Now predict for test set but remember to preprocess it else it will be a disaster\n",
        "predictions = model3.predict(preprocess(X_test))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, predictions)#Confusion metrics\n",
        "\n",
        "#printing errors\n",
        "print((100-sq_err_per(y_test, predictions)))\n",
        "print((100-err_per(y_test, predictions)))\n",
        "print(cm)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78.32512315270935\n",
            "78.32512315270935\n",
            "[[86 26]\n",
            " [18 73]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5-Pfz33bvKw"
      },
      "source": [
        "#Support Vector Machines(84.2%)\n",
        "Generally, Support Vector Machines is considered to be a classification approach, it but can be employed in both types of classification and regression problems. It can easily handle multiple continuous and categorical variables. SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8qwAYvvbuK-",
        "outputId": "bb9ef48f-89a5-4162-8cdf-8a74af3dd8ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "model4=svm.SVC()#SVC is support vector classifier\n",
        "model4.fit(scaled_data,y_train)\n",
        "predictions=model4.predict(preprocess(X_test))\n",
        "\n",
        "cm=metrics.confusion_matrix(y_test,predictions)\n",
        "\n",
        "#printing errors\n",
        "print((100-sq_err_per(y_test, predictions)))\n",
        "print((100-err_per(y_test, predictions)))\n",
        "print(cm)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "85.71428571428572\n",
            "85.71428571428572\n",
            "[[88 24]\n",
            " [ 5 86]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reHq_8Grfhi9"
      },
      "source": [
        "# Decision Trees(81.7%)\n",
        "\n",
        "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MetLTGW5fh14",
        "outputId": "deeb7704-6d16-4f86-b084-81237b907797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "model5=DecisionTreeClassifier(criterion=\"entropy\", max_depth=5)\n",
        "\n",
        "model5.fit(scaled_data,y_train)\n",
        "predictions=model5.predict(preprocess(X_test))\n",
        "\n",
        "cm=metrics.confusion_matrix(y_test,predictions)\n",
        "\n",
        "#printing errors\n",
        "print((100-sq_err_per(y_test, predictions)))\n",
        "print((100-err_per(y_test, predictions)))\n",
        "print(cm)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "81.77339901477832\n",
            "81.77339901477832\n",
            "[[82 30]\n",
            " [ 7 84]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u84-945Bg_uH"
      },
      "source": [
        "#Random forest classifier(94.5%)\n",
        "\n",
        "It technically is an ensemble method (based on the divide-and-conquer approach) of decision trees generated on a randomly split dataset. This collection of decision tree classifiers is also known as the forest. The individual decision trees are generated using an attribute selection indicator such as information gain, gain ratio, and Gini index for each attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgxpR6zzg_0s",
        "outputId": "08edc444-8f3e-4fef-fdc3-f98a5a953e15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "model6=RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "model6.fit(scaled_data,y_train)\n",
        "predictions=model6.predict(preprocess(X_test))\n",
        "\n",
        "cm=metrics.confusion_matrix(y_test,predictions)\n",
        "\n",
        "#printing errors\n",
        "print((100-sq_err_per(y_test, predictions)))\n",
        "print((100-err_per(y_test, predictions)))\n",
        "print(cm)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94.58128078817734\n",
            "94.58128078817734\n",
            "[[106   6]\n",
            " [  5  86]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9sh4VEJidHn"
      },
      "source": [
        "#XGBoost(84%)\n",
        "They are boosting trees. Boosters turn weak learners into strong learners by focusing on where the individual models (usually Decision Trees) went wrong. In Gradient Boosting, individual models train upon the residuals, the difference between the prediction and the actual results. Instead of aggregating trees, gradient boosted trees learns from errors during each boosting round.\n",
        "\n",
        "Basic version is applied here for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcgIC9PKidOc",
        "outputId": "923ba776-7577-4c88-de8e-5aae24c6760e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn import metrics\n",
        "\n",
        "model7=gb_model = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
        "\n",
        "model7.fit(scaled_data,y_train)\n",
        "predictions=model7.predict(preprocess(X_test))\n",
        "\n",
        "cm=metrics.confusion_matrix(y_test,predictions)\n",
        "\n",
        "#printing errors\n",
        "print((100-sq_err_per(y_test, predictions)))\n",
        "print((100-err_per(y_test, predictions)))\n",
        "print(cm)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "84.72906403940887\n",
            "84.72906403940887\n",
            "[[87 25]\n",
            " [ 6 85]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCKSMPFplcc8"
      },
      "source": [
        "#Conclusion\n",
        " \n",
        " On fine tuning scaled data(min-max scaler with some twicks) we reached an accuracy of 94.5% using Random Forest regressor.\n",
        "\n",
        " ## Note \n",
        " #### Accuracy may vary under some parameters"
      ]
    }
  ]
}