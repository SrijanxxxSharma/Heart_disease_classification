{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Heart_disease_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqc/H+V5ooLD4a37e5lkqc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SrijanxxxSharma/Heart_disease_classification/blob/main/Heart_disease_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzqDN1kknUBy"
      },
      "source": [
        "# **Heart disease dataset**\n",
        "\n",
        "### This data set can be found in my Github repository as __.\n",
        "\n",
        "Looking at the data it is quite clear that this is a classification problem(target column is a categorical column). 8 of its columns are categorical(0,1,2,3...) while remaining 5 are continuous numeric column.\n",
        "\n",
        "  Since we don't have a lot of features we won't use any DR(dimension reduction). We'll just scale 5 of our features which are continuous(standardizing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5xvQysxqSE9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "url = \"https://github.com/SrijanxxxSharma/Heart_disease_classification/raw/main/heart-disease-dataset.csv\"\n",
        "raw_data=pd.read_csv(url)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo0rUlh6UI0s"
      },
      "source": [
        "#Now we will use train test split for creating 3 sets\n",
        "1.Training set.(70% of data)\n",
        "\n",
        "2.Test set.(20% of data)\n",
        "\n",
        "3.Validation set.(10% of data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR3GSq6ZUeit",
        "outputId": "4ac90bce-241b-46f5-f708-bd0d3cf6b416",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target=raw_data.target\n",
        "features=raw_data.drop(\"target\",axis=1)\n",
        "\n",
        "# Spliting the data into train and remain sets\n",
        "X_train, X_remain, y_train, y_remain = train_test_split(features, target, test_size=0.30, random_state=42)\n",
        "\n",
        "#spliting test set into test and validation test\n",
        "\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_remain, y_remain, test_size=0.34, random_state=42)\n",
        "X_train.reset_index(inplace=True,drop=True)\n",
        "X_test.reset_index(inplace=True,drop=True)\n",
        "X_val.reset_index(inplace=True,drop=True)\n",
        "y_train=y_train.reset_index(drop=True)\n",
        "y_test=y_test.reset_index(drop=True)\n",
        "y_val=y_val.reset_index(drop=True)\n",
        "\n"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(717, 13) (203, 13) (105, 13)\n",
            "     age  sex  cp  trestbps  chol  ...  exang  oldpeak  slope  ca  thal\n",
            "0     59    1   1       140   221  ...      1      0.0      2   0     2\n",
            "1     58    1   0       128   216  ...      1      2.2      1   3     3\n",
            "2     44    0   2       118   242  ...      0      0.3      1   1     2\n",
            "3     50    1   2       140   233  ...      0      0.6      1   1     3\n",
            "4     43    0   2       122   213  ...      0      0.2      1   0     2\n",
            "..   ...  ...  ..       ...   ...  ...    ...      ...    ...  ..   ...\n",
            "712   41    1   2       130   214  ...      0      2.0      1   0     2\n",
            "713   61    1   0       140   207  ...      1      1.9      2   1     3\n",
            "714   51    1   0       140   299  ...      1      1.6      2   0     3\n",
            "715   43    1   0       110   211  ...      0      0.0      2   0     3\n",
            "716   52    1   0       112   230  ...      0      0.0      2   1     2\n",
            "\n",
            "[717 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJkR5NwD4iiI"
      },
      "source": [
        "#preprocessing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Now we extract continuous variables and convert them to numpy array to scale them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6E3f-rKFb3o"
      },
      "source": [
        "def preprocess(raw_data):\n",
        "  #extracting continuous variables as list of numpy arrays  \n",
        "  continuous_data=[raw_data[members].to_numpy() for members in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']]\n",
        "  \n",
        "  #scaling\n",
        "  name=['Scaled_age', 'Scaled_trestbps', 'Scaled_chol', 'Scaled_thalach', 'Scaled_oldpeak']\n",
        "  i=0\n",
        "  for feature in continuous_data:\n",
        "    new=4*(feature-np.min(feature))/(np.max(feature)-np.min(feature))\n",
        "    #new=(4*feature-feature.mean())/feature.std()\n",
        "    raw_data=pd.concat([raw_data,pd.DataFrame(new, columns = [name[i]])], axis=1)\n",
        "    i+=1\n",
        "  columns=['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
        "  raw_data.drop(columns,inplace=True,axis=1)\n",
        "  return raw_data\n",
        "\n",
        "scaled_data = preprocess(X_train) "
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBaOw2pjsVEl"
      },
      "source": [
        "## Error analysis\n",
        "We use mean squared error to analyse our model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I618hg7LslWf"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.metrics import mean_absolute_error as err\n",
        "def sq_err_per(original,predictions):\n",
        "  return 100*mse(original,predictions)\n",
        "\n",
        "def err_per(original,predictions):\n",
        "  return 100*err(original,predictions)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIkiv4RCcD1W"
      },
      "source": [
        "# Now Model\n",
        "Now we have nicely scaled data as scaled_data. We will use classifiers to train our Model.\n",
        "\n",
        "1.K Nearest Neighbour\n",
        "\n",
        "2.Logistic Regression\n",
        "\n",
        "3.Na√Øve Bayes\n",
        "\n",
        "4.Stochastic Gradient Descent\n",
        "\n",
        "5.Support Vector Machine\n",
        "\n",
        "6.Decision Tree\n",
        "\n",
        "7.Random Forest\n",
        "\n",
        "8.XGboost "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "symiA0ZAeAYX"
      },
      "source": [
        "## K NearestNeighbours\n",
        "\n",
        "This classifier has simple concept. It checks the neighbouring point's label using Eucledean distance. Here K is number if neighbours one wants to consider.\n",
        "\n",
        "#Curse of Dimensionality\n",
        "\n",
        "KNN performs better with a lower number of features than a large number of features. You can say that when the number of features increases than it requires more data. Increase in dimension also leads to the problem of overfitting. To avoid overfitting, the needed data will need to grow exponentially as you increase the number of dimensions. This problem of higher dimension is known as the Curse of Dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztZP9wDlfa-X"
      },
      "source": [
        "\n",
        "# Import relavent model from sklearn\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model = KNeighborsClassifier(n_neighbors=10)\n",
        "\n",
        "# Now fit in each point\n",
        "model.fit(scaled_data,y_train)\n",
        "# Now predict for test set but remember to preprocess it else it will be a disaster\n",
        "predictions = model.predict(preprocess(X_test))\n",
        "\n",
        "#printing errors\n",
        "print((100-sq_err_per(y_test, predictions)))\n",
        "print((100-err_per(y_test, predictions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0qZsurXwJJ1"
      },
      "source": [
        "# Logistic regressor"
      ]
    }
  ]
}